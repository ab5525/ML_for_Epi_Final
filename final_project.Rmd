---
title: "Final_Project"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pacman)

pacman::p_load(pacman, Amelia, caret, cluster, devtools, factoextra, gbm, GGally, ggplot2, ggthemes, 
  ggvis, ggbiplot,httr, knitr,lubridate, plotly, pROC,randomForest, rio, rpart, rpart.plot, rmarkdown, shiny, 
  stringr, stats,tidyr, tidyverse) 

set.seed(123)
```

Introduction: Public health officials have historically had difficulty understanding the reasons that people resist recommendations. During the covid-19 pandemic, vaccinations, the following booster campaigns ran into resistance preventing them from being fully effective. We believe that using data collected from individuals could provide predictions on whom is most likely to be vaccinated, get the disease, and the reasons for each of those. Our models will take into account social factors, risks, and pandemic fear and attempt to provide insight into the behavior of individuals in the population. 

### Load Data

```{r process_data}
df <- import('data/C3longitudinal_clean.csv')
info_cols <- c('STATE','RecSource','age','household_size','air_travel','gender','race')
risk_cols <- c('smokeV0','esmokeV0','antiviral','alcohol_use')
interest_cols <- c('early_vax_interest','delay_side_effects','delay_inconvenient','delay_trust')
                  
children_cols <- c('children_delay_se','children_delay_incon','children_delay_trust')

exposure_cols <- c('swimming_1','park_1','swimming_2','park_2',
                   'movie_1','mall_1','church_1','movie_2','mall_2','church_2')

#removed from above becasue people werent consistent on it 
no_exposure <- c('no_exp','no_exp2')

outcome_cols <- c('had_covid','booster','vaccinated','vax_comb')

risk_exposure <- c(exposure_cols, risk_cols)

fear_cols <- c()

df[,'booster']
df[,interest_cols]
```

```{r Some simplification}
set.seed(123)

df <- na.omit(df)
library(dplyr)


#combine the exposure columns a bit
df <- df %>% mutate(
   delay_any = as.logical(delay_side_effects + delay_inconvenient + delay_trust),
   combined_1 = as.logical(swimming_1 + park_1 + movie_1 + mall_1 + church_1 + air_travel),
   combined_2 = as.logical(swimming_2 + park_2 + movie_2 + mall_2 + church_2),
   combined_any = as.logical(swimming_1 + park_1 + movie_1 + mall_1 + church_1 
                             + swimming_2 + park_2 + movie_2 + mall_2 + church_2 + air_travel),
)

#change logical columns to factors 
#df.lgl <- colnames(df[,sapply(df,is.logical)])
df[,outcome_cols] <- lapply(df[,outcome_cols], factor)
df
```

```{r propensity scoring}
# logistic regression for if someone ever went to exposure locations based on race, age, gender etc.  
ps.model.logit <- glm(combined_any ~ age + gender + race ,
                    data=df, family=binomial(link="logit"))
    summary(ps.model.logit)
    
    # estimates odds of eversmoke, then convert to probability (aka the propensity score)
    prop.score <- (predict(ps.model.logit, df, type="response"))
    df$PS <- prop.score
    
    # Logistic Regression model can be misspecified rather easily. Example, do age and smoking have a linear relationship?
    
   #temp<-table(nmesdata$eversmk, df$LASTAGE) 
    #pct.eversmk <- 100*(temp[2, ] / (temp[2,] + temp[1,]))
    #plot(40:94, pct.eversmk, xlab="Age", ylab="% ever smokers", pch=".", cex=7, cex.lab=1.5, cex.axis=1.5)
```
```{r}
train.index<-createDataPartition(y=df$vaccinated,p=0.7,list=FALSE)
fdf.train <- df[train.index,]
fdf.test <- df[-train.index,]



df1 <- df[, c(no_exposure,risk_exposure,'PS','had_covid','vaccinated')]
#df1 <- df[, c("combined_any",no_exposure,risk_cols,'PS','had_covid','vaccinated')]

train.index<-createDataPartition(y=df$had_covid,p=0.7,list=FALSE)
df.train <- df1[train.index,]
df.test <- df1[-train.index,]


df1 <- df[, c(risk_exposure,interest_cols,'PS','vaccinated')]
train.index2<-createDataPartition(y=df$vaccinated,p=0.7,list=FALSE)
df.train2 <- df1[train.index2,]
df.test2 <- df1[-train.index2,]


summary(df.train)
```


### Unsupervised learning and clustering

We were also interested in grouping participants by their interest in getting the vaccine. There are multiple components that contribute to vaccine interest, which include their early interest in the vaccine, whether they would or would not delay getting the vaccine for various reasons, and whether they would or would not delay getting their children the vaccine for various reasons.

First, we needed to find the optimal number of clusters. There are several statistics that can be used to find the optimal number, and we chose the silhouette method which compares the similarity of each point to other points in the cluster compared to other clusters.

```{r unsupervised}
# calculate distance matrix
dist.matrix <- dist(df[,c(interest_cols, children_cols)], method="euclidean")
# hierarchical clustering
h <- hclust(dist.matrix, method="complete")
plot(h)

# cut trees into various clusters and calculate silhouette score
silhouette_score <- function(k){
    cluster <- cutree(tree=h, k=k)
    ss <- silhouette(cluster, dist.matrix)
    mean(ss[, 3])
}

k <- 2:20

# calculate average silhouette score
avg_sil <- sapply(k, silhouette_score)
sil_df <- data.frame(Clusters=k, Score=avg_sil)
p <- ggplot(sil_df, mapping=aes(x=Clusters, y=Score)) + geom_point(size=3) + labs(x="Number of Clusters", y="Silhouette Score") +
          theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = 'white'))

print(p)
```

Here, our plot shows that 2 clusters has the highest silhouette score. Although this is technically the best separation it is not that informative (i.e., the two clusters are most likely willing vs non-willing to get the vaccine). It is more interesting to divide into 8 clusters, which has the next highest silhouette score and shows a marked increase in score from 7 clusters.

```{r}
clustering <- cutree(tree=h, k=8)
clustering <- as.factor(clustering)
df$interest_cluster <- clustering
### each cluster has a different "level of interest"
```

*Cluster 1*
Cluster 1 appears to have the the highest level of interest. They generally have early interest in getting vaccinated, and will not delay the vaccine for either themselves or their children, for any reason. This is the largest cluster.

```{r}
summary(df[df$interest_cluster == 1, c(interest_cols, children_cols)])
```

*Cluster 2*
Cluster 2 has an early interest in the vaccine, although they would delay the vaccine for themselves for fear of side effects or inconvenience (but not as much due to mistrust). However, they will not delay the vaccine for their children. 

```{r}
summary(df[df$interest_cluster == 2, c(interest_cols, children_cols)])
```

*Other Clusters*
Other clusters have different patterns for how willing they are to get the vaccine, but they are minor in numbers so we will not go much deeper into them.


```{r logistic regression}
### logistic regression predictions
logit <- glm(had_covid ~ PS  + antiviral + booster+alcohol_use + air_travel + combined_2,
                    data=df, family=binomial(link="logit"))
summary(logit)

control.obj<-trainControl(method="cv", number=5)
fam <- 'binomial'

set.seed(123)
logit<-train(vaccinated~ PS + hoax + early_vax_interest +antiviral+combined_any, data=fdf.train, method="glm", family=fam, preProcess=c("center", "scale"), trControl=control.obj)

logit$results
confusionMatrix(logit)
coef(logit$finalModel)

logit.prob<- logit %>% predict(fdf.test)

cm = confusionMatrix(logit.prob, fdf.test$vaccinated, positive = 'TRUE')
cm

###### Had covid
control.obj<-trainControl(method="cv", number=5)

set.seed(123)
logit<-train(had_covid~ PS + hoax+ booster + antiviral + early_vax_interest+alcohol_use + air_travel + combined_any, data=fdf.train, method="glm", family=fam, preProcess=c("center", "scale"), trControl=control.obj)

logit$results
confusionMatrix(logit)
coef(logit$finalModel)

logit.prob<- logit %>% predict(fdf.test)

cm = confusionMatrix(logit.prob, fdf.test$had_covid, positive = 'TRUE')
cm
```
```{r svm attempt}
set.seed(123)

control.obj<-trainControl(method="cv", number=5)

svc.model<-train(vaccinated~ PS  + early_vax_interest +antiviral+combined_1, data=fdf.train, method="svmLinear", trControl=control.obj, preProcess=c("center", "scale"), probability=TRUE, tuneGrid=expand.grid(C=seq(0.0001,100, length=5)))

svc.model$bestTune
svc.model$results
confusionMatrix(svc.model)

svc.prob<- svc.model %>% predict(fdf.test)
cm = confusionMatrix(svc.prob, fdf.test$vaccinated, positive = 'TRUE')
cm
```

```{r random forest tuning}
train.control<-trainControl(method="cv", number=5, summaryFunction = twoClassSummary, sampling="down")

tree.grid<-expand.grid(cp=seq(0.000001, 0.001, by=0.00005))

tree.model <-train(had_covid~., data=df.train, method="rpart",trControl=train.control, tuneGrid=tree.grid, metric="Sens")

tree.model$bestTune
rpart.plot(tree.model$finalModel)

confusionMatrix(tree.model)
```

```{r}

feat.count<-c((ncol(df.train)-1)/2, sqrt(ncol(df.train)-1), (ncol(df.train)-1))

grid.rf<-expand.grid(mtry=feat.count)

control.obj<-trainControl(method="cv", number=5, sampling = 'up')

tree.num<-seq(100,300, by=100)
results.trees<-list()
for (ntree in tree.num){
  set.seed(123)
    rf.model<-train(had_covid~., data=df.train, method="rf", trControl=control.obj, metric="Accuracy", tuneGrid=grid.rf, importance=TRUE, ntree=ntree)
    index<-toString(ntree)
  results.trees[[index]]<-rf.model$results
}

output <-bind_rows(results.trees, .id = "ntrees")
best.tune<-output[which.max(output[,"Accuracy"]),]
best.tune$mtry
results.trees
mtry.grid<-expand.grid(.mtry=best.tune$mtry)

######
#predicting vaccination
feat.count2<-c(sqrt(ncol(df.train2)-1), log(ncol(df.train2)-1))
grid.rf2<-expand.grid(mtry=feat.count2)
control.obj2<-trainControl(method="cv", number=5)

tree.num<-seq(100,300, by=100)
results.trees2<-list()

for (ntree in tree.num){
  set.seed(123)
    rf.model2<-train(vaccinated~ PS + hoax + antiviral + early_vax_interest + combined_1, data=fdf.train, method="rf", trControl=control.obj2, metric="Accuracy", tuneGrid=grid.rf2, importance=TRUE, ntree=ntree)
    index<-toString(ntree)
  results.trees2[[index]]<-rf.model2$results
}

output2 <-bind_rows(results.trees2, .id = "ntrees")
best.tune2<-output2[which.max(output2[,"Accuracy"]),]
best.tune2$mtry
results.trees2
mtry.grid2<-expand.grid(.mtry=best.tune2$mtry)

```

```{r random forest model}
set.seed(123)
    rf.model<-train(had_covid~., data=df.train, method="rf", trControl=control.obj, metric="Accuracy", tuneGrid=mtry.grid, importance=TRUE,ntree=as.numeric(best.tune$ntrees))

confusionMatrix(rf.model)
varImp(rf.model)

######
#Model for vaccinated 
set.seed(123)
    rf.model2<-train(vaccinated~ age + race + hoax + early_vax_interest + combined_any, data=fdf.train, method="rf", trControl=control.obj2, metric="Accuracy", tuneGrid=mtry.grid2, importance=TRUE,ntree=as.numeric(best.tune2$ntrees))

confusionMatrix(rf.model2)
varImp(rf.model2)
```
# Prediction on test set 

```{r test_set}

#Predict in test-set and output probabilities
rf.preds<- rf.model %>% predict(df.test)
rf.preds2<- predict(rf.model, df.test, type="prob")

#Pull out predicted probabilities for Diabetes=Yes
rf.probability<-rf.preds2[,2]

cm = confusionMatrix(rf.preds, df.test$had_covid, positive = 'TRUE')
cm
'
#Predict in test-set and output probabilities
rf.preds<- rf.model2 %>% predict(df.test2)
rf.preds2<- predict(rf.model2, df.test2, type="prob")

#Pull out predicted probabilities for Diabetes=Yes
rf.probability<-rf.preds2[,2]
'

#cm = confusionMatrix(rf.preds, df.test2$vaccinated, positive = 'TRUE')
#cm

rf.preds<- rf.model2 %>% predict(fdf.test)
rf.preds2<- predict(rf.model2, fdf.test, type="prob")

#Pull out predicted probabilities for Diabetes=Yes
rf.probability<-rf.preds2[,2]

cm = confusionMatrix(rf.preds, fdf.test$vaccinated, positive = 'TRUE')
cm

```


# plotting
```{r plotting_calibration}
pred.prob<-data.frame(Class=df.test$had_covid, rf=rf.probability)

calplot<-(calibration(Class ~ rf, data=pred.prob, class=TRUE, cuts=10))

xyplot(calplot, auto.key=list(columns=1))



cal.data.index<-df.test$had_covid%>% createDataPartition(p=0.5, list=F)
cal.data<-df.test[cal.data.index, ]
final.test.data<-df.test[-cal.data.index, ]
#Predict on test-set without scaling to obtain raw pred prob in test set
rf.probs.nocal<-predict(rf.model, final.test.data, type="prob")
rf.pp.nocal<-rf.probs.nocal[,2]

#Apply model developed on training data to calibration dataset to obtain predictions
rf.probs.cal<-predict(rf.model, cal.data, type="prob")
rf.pp.cal<-rf.probs.cal[,2]

#Add to dataset with actual values from calibration data
calibrf.data.frame<-data.frame(rf.pp.cal, cal.data$had_covid)
colnames(calibrf.data.frame)<-c("x", "y")

#Use logistic regression to model predicted probabilities from calibration data to actual vales
calibrf.model<-glm(y ~ x, data=calibrf.data.frame, family = binomial)

#Apply calibration model above to raw predicted probabilities from test set
data.test.rf<-data.frame(rf.pp.nocal)
colnames(data.test.rf)<-c("x")
platt.data.rf<-predict(calibrf.model, data.test.rf, type="response")

platt.prob.rf<-data.frame(Class=final.test.data$had_covid, rf.platt=platt.data.rf, rf=rf.pp.nocal)

calplot.rf<-(calibration(Class ~ rf.platt+rf, data=platt.prob.rf, class=TRUE, cuts=10))
xyplot(calplot.rf, auto.key=list(columns=2))
```

**Limitations**
One of the limitations of our project is that the data itself was very difficult to process. Our data set was particularly large because it was a compilation of several surveys taken at different times throughout the pandemic. As a result, many of the questions we were interested in had several versions corresponding to different time points. In effect, this made it difficult to analyze the data because there was a high rate of dropout, where we would have less people responding to the newer version of the question compared to an older one. When this happens, we needed to make several assumptions to process the data. As an example, the question "in the past month, have you spent time in a mall?", was asked in 9 different surveys, and we chose to combine the results of only the 2 most recent surveys. We would consider a person as being exposed from the mall if the participant answered "Yes" in either survey. However, this assumes that a participant saying "Yes" in either has the same exposure risk as someone saying "Yes" in both surveys, which is probably not the case in real life.

Also, the high rate of dropout also forced us to lose a large portion of our participants. We also acknowledge that the dropout/missing data may not necessarily be at random, which could possibly introduce selection bias into our results. The combination of high dropout, missing values, and unorganized data (requiring assumptions for processing) affects our ability to predict.

**Ethical Considerations**
The ethical considerations that our model has to deal with are racial biases and fair use of the model in the real world. The model that we built uses a data comprising of 65% White, compared to around 15% Hispanic and 7% Black. While this is more or less similar to the demographics across the entire United States, this model might not be applicable when looking at specific populations or groups of people, as individual groups may be more homogeneous than the entire US population. 

This becomes an ethical problem when we try to apply our model into populations where the demographics vary greatly from the demographics used in our model. For example, we might unintentionally underpredict the rate of vaccination for a population, which may cause an undersupply of vaccinations for the people who probably need them most. It is also ethically troubling if we use this model to target specific populations simply because they are predicted to have lower or higher rates of vaccination - this would raise the question of whether the vaccine distribution is fair.

In the same vein, our model predicts whether a person will get vaccinated or not, but the true public health problem at hand is to equitably distribute the vaccines. We need to be able to distinguish predicting a person's status of vaccination from that person's need for vaccination. So, we need to exercise lots of caution to translate predictability to real public health measures. 